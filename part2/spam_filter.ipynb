{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 2 of Getting Started with Natural Language Processing\n",
    "by Ekaterina Kochmar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing\n",
    "\n",
    "Looking at some methods to split textual data into meaningful words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using split to seperate whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Michael', 'Jordan', 'is', 'considered', 'by', 'many', 'to', 'be', 'the', 'greatest', 'basketball', 'player', 'of', 'all', 'time']\n"
     ]
    }
   ],
   "source": [
    "text = 'Michael Jordan is considered by many to be the greatest basketball player of all time'\n",
    "\n",
    "text_words = text.split(\" \")\n",
    "print(text_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Considering punctuation\n",
    "Punctuation will be considered as part of the word which changes the word representation as a feature as opposed to the *same* word that has no punctuation. This will cause issues for NLP and one which tokenization should handle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['How', 'should', 'we', 'regard', \"Jordan's\", '\"greatness\"', 'in', 'a', 'basketball', 'context?', 'Is', 'it', 'Greatness', 'at', 'some', 'or', 'all', 'facets', 'of', 'the', 'game?', 'Does', 'the', 'U.S.A', 'have', 'different', 'criteria', 'to', 'U.K?', 'First,', 'lets', 'consider', \"Jordan's\", '', 'greatness.', '', 'How', 'great', 'was', 'Jordan?', '', \"I'm\", 'going', 'to', 'say', 'he', 'was', 'great!', '', 'I', \"haven't\", 'met', 'anyone', 'who', 'disagrees.']\n"
     ]
    }
   ],
   "source": [
    "text = 'How should we regard Jordan\\'s \"greatness\" in a basketball context? Is it Greatness at \\\n",
    "some or all facets of the game? Does the U.S.A have different criteria to U.K? First, lets consider \\\n",
    "Jordan\\'s  greatness. '\n",
    "\n",
    "question = \"How great was Jordan? \"\n",
    "answer = \"I'm going to say he was great! \"\n",
    "answer2 = \"I haven't met anyone who disagrees.\"\n",
    "text = text +\" \"+ question +\" \"+ answer +\" \"+ answer2\n",
    "\n",
    "text_words = text.split(\" \")\n",
    "print(text_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In sentence above **\"greatness\"**, **Greatness** and **greatness** all have the same meaning but will be represented differenly as words. The capitalisation of words will be considered when we normalise textual data.\n",
    "\n",
    "Lets update the algorithm so that it considers puntuation when splitting the text into words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How should we regard Jordan\\'s \"greatness\" in a basketball context? Is it Greatness at some or all facets of the game? Does the U.S.A have different criteria to U.K? First, lets consider Jordan\\'s  greatness.  How great was Jordan?  I\\'m going to say he was great!  I haven\\'t met anyone who disagrees.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# List of words\n",
    "words = []\n",
    "\n",
    "# Track the current word\n",
    "current_word = \"\"\n",
    "\n",
    "# List of delimiters\n",
    "delimiters = ['\"', '.', \"?\", \"!\"]\n",
    "\n",
    "# iterate through each character in the text\n",
    "for c in text:\n",
    "    if len(current_word) > 0:\n",
    "        previous_char = current_word[-1]\n",
    "    \n",
    "    if c == \" \":\n",
    "        words.append(current_word)    # add the current_word to words list\n",
    "        current_word = \"\"             # initialise the current_word to empty string again\n",
    "    elif c in delimiters and previous_char != \" \":\n",
    "        words.append(current_word)\n",
    "        words.append(c)\n",
    "        current_word = \"\"\n",
    "    elif c in delimiters and previous_char == \" \":\n",
    "        words.append(c)\n",
    "        current_word = \"\"\n",
    "    else:\n",
    "        current_word += c \n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['How', 'should', 'we', 'regard', \"Jordan's\", '', '\"', 'greatness', '\"', '', 'in', 'a', 'basketball', 'context', '?', '', 'Is', 'it', 'Greatness', 'at', 'some', 'or', 'all', 'facets', 'of', 'the', 'game', '?', '', 'Does', 'the', 'U', '.', 'S', '.', 'A', 'have', 'different', 'criteria', 'to', 'U', '.', 'K', '?', '', 'First,', 'lets', 'consider', \"Jordan's\", '', 'greatness', '.', '', '', 'How', 'great', 'was', 'Jordan', '?', '', '', \"I'm\", 'going', 'to', 'say', 'he', 'was', 'great', '!', '', '', 'I', \"haven't\", 'met', 'anyone', 'who', 'disagrees', '.']\n"
     ]
    }
   ],
   "source": [
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From the book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['How', 'should', 'we', 'regard', \"Jordan's\", '\"', 'greatness', '\"', 'in', 'a', 'basketball', 'context', '?', 'Is', 'it', 'Greatness', 'at', 'some', 'or', 'all', 'facets', 'of', 'the', 'game', '?', 'Does', 'the', 'U', '.', 'S', '.', 'A', 'have', 'different', 'criteria', 'to', 'U', '.', 'K', '?', 'First,', 'lets', 'consider', \"Jordan's\", 'greatness', '.', 'How', 'great', 'was', 'Jordan', '?', \"I'm\", 'going', 'to', 'say', 'he', 'was', 'great', '!', 'I', \"haven't\", 'met', 'anyone', 'who', 'disagrees', '.']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "delimiters = ['\"', \".\", \"?\", \"!\"]\n",
    "words = []\n",
    "current_word = \"\"\n",
    " \n",
    "for char in text:\n",
    "    if char == \" \":\n",
    "        if not current_word == \"\":\n",
    "            words.append(current_word)\n",
    "            current_word = \"\"\n",
    "    elif char in delimiters:\n",
    "        if current_word == \"\":\n",
    "            words.append(char)\n",
    "        else:\n",
    "            words.append(current_word)\n",
    "            words.append(char)\n",
    "            current_word = \"\"\n",
    "    else:\n",
    "        current_word += char\n",
    "        \n",
    "    \n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are still some issues with the above code if they are to operate as tokenizers:\n",
    "* Can't handle abbreviations like \"U.K.\" or \"i.e.\"\n",
    "* Word concatenation like \"I'm\" will not be understood as \"I am\"\n",
    "    * Tokenizer would split the answer into [I, 'm, going, to , say, he ,was, great, !] and some rules would recognise 'm as short for \"am\" in this example\n",
    "    * Similarly \"haven't\" should be recognised as \"have not\". Tokenizer would split answer2 into [I have, n't, met, anyone, who, disagees, .]\n",
    "\n",
    "Good tokenizing packages ensure that text is properly split into words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enron case study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import codecs\n",
    "\n",
    "# ISO-8859-1: alias for latin-1 codec which is the language system for western europe\n",
    "# Enron emails are in English\n",
    "\n",
    "def read_in(folder):\n",
    "    files = os.listdir(folder)\n",
    "    a_list = []\n",
    "    for file in files:\n",
    "        if not file.startswith(\".\"):\n",
    "            f = codecs.open(os.path.join(folder,file), \"r\", \n",
    "                encoding=\"ISO-8859-1\", errors=\"ignore\")\n",
    "            a_list.append(f.read())\n",
    "            f.close()\n",
    "    return a_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500\n",
      "3672\n",
      "***SPAM***\n",
      "Subject: last notice\r\n",
      "problem mount sure pattern . art , were do any . give cloud had , it\r\n",
      "noun us may . their such let past , part sound . stand buy through ,\r\n",
      "every . leave say current . wear rest , blow hair final word\r\n",
      "before . way talk quick was . seem this though talk live wild\r\n",
      "problem . map opposite able , sleep put world .\r\n",
      "- -\r\n",
      "phone : 837 - 444 - 1269\r\n",
      "mobile : 268 - 464 - 9520\r\n",
      "email : yorkers @ alltel . net\r\n",
      "\n",
      "***HAM***\n",
      "Subject: enron / hpl actuals for november 27 , 2000\r\n",
      "teco tap 20 . 000 / enron ; 100 . 000 / hpl gas daily\n"
     ]
    }
   ],
   "source": [
    "data_path = 'data/enron1/'\n",
    "\n",
    "# each element in the list contains contents of one email\n",
    "ham_list = read_in(os.path.join(data_path, 'ham'))\n",
    "spam_list = read_in(os.path.join(data_path, 'spam'))\n",
    "\n",
    "print(len(spam_list))\n",
    "print(len(ham_list))\n",
    "print(\"***SPAM***\")\n",
    "print(spam_list[1])\n",
    "print(\"***HAM***\")\n",
    "print(ham_list[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing the data\n",
    "Combine the ham/spam datasets, include the label then shuffle the final dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Subject: last notice\\r\\nproblem mount sure pattern . art , were do any . give cloud had , it\\r\\nnoun us may . their such let past , part sound . stand buy through ,\\r\\nevery . leave say current . wear rest , blow hair final word\\r\\nbefore . way talk quick was . seem this though talk live wild\\r\\nproblem . map opposite able , sleep put world .\\r\\n- -\\r\\nphone : 837 - 444 - 1269\\r\\nmobile : 268 - 464 - 9520\\r\\nemail : yorkers @ alltel . net\\r\\n', 'spam')\n",
      "Dataset size = 5172 emails\n"
     ]
    }
   ],
   "source": [
    "import random \n",
    "\n",
    "all_emails = [(email_content, \"spam\") for email_content in spam_list]\n",
    "all_emails += [(email_content, \"ham\") for email_content in ham_list]\n",
    "print(all_emails[1])\n",
    "\n",
    "# Note\n",
    "# From lib/python3.8/string.py\n",
    "# whitespace -- a string containing all ASCII whitespace\n",
    "# Some strings for ctype-style character classification\n",
    "# whitespace = ' \\t\\n\\r\\v\\f'\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(all_emails)\n",
    "print(f\"Dataset size = {str(len(all_emails))} emails\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizing\n",
    "Emails are single string of symbols so need to split the text into words. Use NLTK's tokenizer for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jack\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jack/repos/hosting-ml-\n",
      "[nltk_data]     as-service/venv/share/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk \n",
    "\n",
    "user = os.environ[\"USER\"]\n",
    "print(user)\n",
    "\n",
    "nltk.download('punkt', download_dir='/home/'+user+'/repos/hosting-ml-as-service/venv/share/nltk_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['What', \"'s\", 'the', 'best', 'way', 'to', 'split', 'a', 'sentence', 'into', 'words', '?']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from nltk import word_tokenize\n",
    "\n",
    "def tokenize(input):\n",
    "    word_list = []\n",
    "    for word in word_tokenize(input):\n",
    "        word_list.append(word)\n",
    "    return word_list\n",
    "\n",
    "input = \"What's the best way to split a sentence into words?\"\n",
    "print(tokenize(input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract and normalise the features\n",
    "* Iterate over emails and tokenize the text that has been tranformed to be lower case so that it is normalised. \n",
    "* Each email and label are paired together in tuples.\n",
    "* Tokenized list of words are converted to dictionary with words as keys and set to value of True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(text):\n",
    "    features = {}\n",
    "    word_list = [word for word in word_tokenize(text.lower())]\n",
    "    for word in word_list:\n",
    "        features[word] = True    # switch on the flag that this word is contained in th\n",
    "    return features\n",
    "\n",
    "all_features = [(get_features(email), label) for (email, label) in all_emails]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"Subject: no doctor ' s visit .\\r\\nstop wasting money on prescription drugs . get them online for 80 % off .\\r\\nvlagra , clalis , zyban , prozac , xenlcal , and many many more . . .\\r\\nstop paying more than you have too !\\r\\n- todays special -\\r\\nviagra , retail price $ 15 . 99 , our price $ 2 . 99\\r\\ncialis , retail price $ 17 . 99 , our price $ 3 . 99\\r\\nshipped world wide\\r\\nno prescription required\\r\\nvisit us here : http : / / imkjbest . com / z /\\r\\n\",\n",
       " 'spam')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_emails[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'subject': True, ':': True, 'no': True, 'doctor': True, \"'\": True, 's': True, 'visit': True, '.': True, 'stop': True, 'wasting': True, 'money': True, 'on': True, 'prescription': True, 'drugs': True, 'get': True, 'them': True, 'online': True, 'for': True, '80': True, '%': True, 'off': True, 'vlagra': True, ',': True, 'clalis': True, 'zyban': True, 'prozac': True, 'xenlcal': True, 'and': True, 'many': True, 'more': True, 'paying': True, 'than': True, 'you': True, 'have': True, 'too': True, '!': True, '-': True, 'todays': True, 'special': True, 'viagra': True, 'retail': True, 'price': True, '$': True, '15': True, '99': True, 'our': True, '2': True, 'cialis': True, '17': True, '3': True, 'shipped': True, 'world': True, 'wide': True, 'required': True, 'us': True, 'here': True, 'http': True, '/': True, 'imkjbest': True, 'com': True, 'z': True}, 'spam')\n"
     ]
    }
   ],
   "source": [
    "print(all_features[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"subject\": true,\n",
      "    \":\": true,\n",
      "    \"no\": true,\n",
      "    \"doctor\": true,\n",
      "    \"'\": true,\n",
      "    \"s\": true,\n",
      "    \"visit\": true,\n",
      "    \".\": true,\n",
      "    \"stop\": true,\n",
      "    \"wasting\": true,\n",
      "    \"money\": true,\n",
      "    \"on\": true,\n",
      "    \"prescription\": true,\n",
      "    \"drugs\": true,\n",
      "    \"get\": true,\n",
      "    \"them\": true,\n",
      "    \"online\": true,\n",
      "    \"for\": true,\n",
      "    \"80\": true,\n",
      "    \"%\": true,\n",
      "    \"off\": true,\n",
      "    \"vlagra\": true,\n",
      "    \",\": true,\n",
      "    \"clalis\": true,\n",
      "    \"zyban\": true,\n",
      "    \"prozac\": true,\n",
      "    \"xenlcal\": true,\n",
      "    \"and\": true,\n",
      "    \"many\": true,\n",
      "    \"more\": true,\n",
      "    \"paying\": true,\n",
      "    \"than\": true,\n",
      "    \"you\": true,\n",
      "    \"have\": true,\n",
      "    \"too\": true,\n",
      "    \"!\": true,\n",
      "    \"-\": true,\n",
      "    \"todays\": true,\n",
      "    \"special\": true,\n",
      "    \"viagra\": true,\n",
      "    \"retail\": true,\n",
      "    \"price\": true,\n",
      "    \"$\": true,\n",
      "    \"15\": true,\n",
      "    \"99\": true,\n",
      "    \"our\": true,\n",
      "    \"2\": true,\n",
      "    \"cialis\": true,\n",
      "    \"17\": true,\n",
      "    \"3\": true,\n",
      "    \"shipped\": true,\n",
      "    \"world\": true,\n",
      "    \"wide\": true,\n",
      "    \"required\": true,\n",
      "    \"us\": true,\n",
      "    \"here\": true,\n",
      "    \"http\": true,\n",
      "    \"/\": true,\n",
      "    \"imkjbest\": true,\n",
      "    \"com\": true,\n",
      "    \"z\": true\n",
      "}\n",
      "----------\n",
      "label: spam\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Representation of first feature\n",
    "a = json.dumps(all_features[0][0], indent=4)\n",
    "\n",
    "print(a)\n",
    "print(\"----------\")\n",
    "print(f\"label: {all_features[0][1]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the classifier\n",
    "\n",
    "#### Naive Bayes\n",
    "\n",
    "Use the Bayes formula to calculate\n",
    "\n",
    "```\n",
    "P(C=c|E) = P(C=c)/P(E) P(E|C=c)\n",
    "```\n",
    "* `P(C=c)`: Probability of the class being equal to c\n",
    "* `P(E)`: Likelihood of the evidence\n",
    "* `P(E|C=c)`: Likelihood of seeing evidence when class is C=c\n",
    "    * If there is a data generating process behind the instance an instance of C=c, how often would that instance look like E?\n",
    "\n",
    "#### Likelihood of the evidence\n",
    "\n",
    "```    \n",
    "P(E|C=c) = N(E when C=c) / N(C=c)\n",
    "```\n",
    "\n",
    "Bayes classifier considered Naive as it make assumption of conditional independence\n",
    "\n",
    "**Note:**\n",
    "We have transformed email content from something like\n",
    "\n",
    "``` \"Participate in our new lottery now!\" ```  \n",
    "\n",
    "to  \n",
    "\n",
    "```['Participate': True, 'in': True,..., '!': True]``` \n",
    "    \n",
    "so that this vectore represents the feature instance and 'Spam' is the class label\n",
    "\n",
    "Conditional independence is saying that each word in our feature vector is independent from one another, so \"new\" is independent of \"lottery\", in this example.\n",
    "\n",
    "Therefore we can re-write `P(E|C=c)`, or in this example   \n",
    "\n",
    "```['Participate': True, 'in': True,..., '!': True | C='Spam']```\n",
    "\n",
    "to\n",
    "\n",
    "```['Participate': True | C='Spam', 'in': True | C='Spam',..., '!' | C='Spam': True]```\n",
    "\n",
    "More generally, if feature vector represented as vector `E = [e1, e2,..., ek]` then:  \n",
    "\n",
    "```P(e1 ⋀ e2 ⋀ ... ⋀ ek | C=c) == P(e1 | C=c) * P(e2 | C=c) *...* P(ek | C=c)  ```\n",
    "\n",
    "Each term can be estimated from the training data \n",
    "\n",
    "```P(e1 | C=c) = N(instances with feature present in Class c) / N(instances in Class c)```\n",
    "\n",
    "#### Prediction\n",
    "\n",
    "Generally, if there are two classes in the training data we predict a class following this psuedocode\n",
    "\n",
    "```\n",
    "for each instance\n",
    "    for each class\n",
    "        calculate probabilites\n",
    "        Estimate P(C=cj|Ei)\n",
    "    if P(c1 | Ei) > P(c2 | Ei)\n",
    "        Predict(c1)\n",
    "    else\n",
    "        Predict(c2)\n",
    "```\n",
    "**Note:** P(E) is cancelled out when comparing each class probability so no need to calculate in this case\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size = 4137 emails\n",
      "Test set size = 1035 emails\n"
     ]
    }
   ],
   "source": [
    "from nltk import NaiveBayesClassifier, classify\n",
    "\n",
    "def train(features, proportion):\n",
    "    train_size = int(len(features) * proportion)\n",
    "    train_set, test_set = features[:train_size], features[train_size:]\n",
    "    print(f\"Training set size = {str(len(train_set))} emails\")\n",
    "    print(f\"Test set size = {str(len(test_set))} emails\")\n",
    "    classifier = NaiveBayesClassifier.train(train_set)\n",
    "    return train_set, test_set, classifier\n",
    "\n",
    "train_set, test_set, classifier = train(all_features, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc = 0.9579405366207396\n",
      "Test acc = 0.9342995169082126\n",
      "Most Informative Features\n",
      "               forwarded = True              ham : spam   =    203.2 : 1.0\n",
      "                     hou = True              ham : spam   =    196.3 : 1.0\n",
      "                    2004 = True             spam : ham    =    159.8 : 1.0\n",
      "            prescription = True             spam : ham    =    129.3 : 1.0\n",
      "                    pain = True             spam : ham    =    100.4 : 1.0\n",
      "                     ect = True              ham : spam   =     86.7 : 1.0\n",
      "                   cheap = True             spam : ham    =     84.3 : 1.0\n",
      "                  farmer = True              ham : spam   =     84.3 : 1.0\n",
      "                     sex = True             spam : ham    =     82.7 : 1.0\n",
      "                featured = True             spam : ham    =     79.5 : 1.0\n",
      "                    2001 = True              ham : spam   =     76.2 : 1.0\n",
      "              nomination = True              ham : spam   =     72.6 : 1.0\n",
      "                   adobe = True             spam : ham    =     71.5 : 1.0\n",
      "                creative = True             spam : ham    =     68.3 : 1.0\n",
      "             medications = True             spam : ham    =     68.3 : 1.0\n",
      "                   risks = True             spam : ham    =     66.7 : 1.0\n",
      "             subscribers = True             spam : ham    =     63.4 : 1.0\n",
      "                    2005 = True             spam : ham    =     63.1 : 1.0\n",
      "                     ali = True             spam : ham    =     61.8 : 1.0\n",
      "                     ibm = True             spam : ham    =     61.8 : 1.0\n",
      "                     pro = True             spam : ham    =     61.8 : 1.0\n",
      "                congress = True             spam : ham    =     57.0 : 1.0\n",
      "                  unique = True             spam : ham    =     57.0 : 1.0\n",
      "                    draw = True             spam : ham    =     55.4 : 1.0\n",
      "                   epson = True             spam : ham    =     55.4 : 1.0\n",
      "              complaints = True             spam : ham    =     53.8 : 1.0\n",
      "                   cisco = True             spam : ham    =     52.2 : 1.0\n",
      "                 dealers = True             spam : ham    =     50.6 : 1.0\n",
      "                thousand = True             spam : ham    =     50.6 : 1.0\n",
      "                deciding = True             spam : ham    =     49.0 : 1.0\n",
      "                  sexual = True             spam : ham    =     49.0 : 1.0\n",
      "                 foresee = True             spam : ham    =     47.4 : 1.0\n",
      "                  health = True             spam : ham    =     43.9 : 1.0\n",
      "                 beliefs = True             spam : ham    =     42.6 : 1.0\n",
      "                    lisa = True              ham : spam   =     41.4 : 1.0\n",
      "                      cc = True              ham : spam   =     41.1 : 1.0\n",
      "                 advises = True             spam : ham    =     41.0 : 1.0\n",
      "                     fat = True             spam : ham    =     41.0 : 1.0\n",
      "                    font = True             spam : ham    =     41.0 : 1.0\n",
      "                  stocks = True             spam : ham    =     40.3 : 1.0\n",
      "                      ex = True             spam : ham    =     40.0 : 1.0\n",
      "                     mix = True             spam : ham    =     39.4 : 1.0\n",
      "                   steve = True              ham : spam   =     39.2 : 1.0\n",
      "                   susan = True              ham : spam   =     39.2 : 1.0\n",
      "                     713 = True              ham : spam   =     38.8 : 1.0\n",
      "                 popular = True             spam : ham    =     38.1 : 1.0\n",
      "                  shares = True             spam : ham    =     38.1 : 1.0\n",
      "              aggressive = True             spam : ham    =     37.7 : 1.0\n",
      "            consultation = True             spam : ham    =     37.7 : 1.0\n",
      "                 doctors = True             spam : ham    =     37.7 : 1.0\n"
     ]
    }
   ],
   "source": [
    "def evaluate(train_set, test_set, classifier):\n",
    "    print(f\"Train acc = {str(classify.accuracy(classifier, train_set))}\")\n",
    "    print(f\"Test acc = {str(classify.accuracy(classifier, test_set))}\")\n",
    "    \n",
    "    # basically the features with biggest difference between P(feature | spam) and P(feature | ham)\n",
    "    # e.g  for Spam: max[P(word:True | spam) / P(word:True | ham)]\n",
    "    classifier.show_most_informative_features(50)\n",
    "    \n",
    "evaluate(train_set, test_set, classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word occurrences\n",
    "\n",
    "Check word occurrence in all available contexts\n",
    "\n",
    "e.g. \"stocks\" is a strong predictor of spam. Why?  \n",
    "```\n",
    "stocks = True             spam : ham    =     40.3 : 1.0\n",
    "```\n",
    "\n",
    "\"stocks\" must be used in a harmless way --> lets check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STOCKS in HAM: \n",
      "Displaying 1 of 1 matches:\n",
      "ur member directory . * follow your stocks and news headlines , exchange files\n",
      "Displaying 1 of 1 matches:\n",
      "ur member directory . * follow your stocks and news headlines , exchange files\n",
      "Displaying 1 of 1 matches:\n",
      "ur member directory . * follow your stocks and news headlines , exchange files\n",
      "Displaying 1 of 1 matches:\n",
      "ad my portfolio is diversified into stocks that have lost even more money than\n",
      "\n",
      "\n",
      "STOCKS in SPAM: \n",
      "Displaying 1 of 1 matches:\n",
      "dge - ksige are you tired of buying stocks and not having them perform ? our s\n",
      "Displaying 5 of 5 matches:\n",
      "hursday ! some of these littie voip stocks have been realiy moving lateiy . an\n",
      "t can happen with these sma | | cap stocks when they take off . and it happens\n",
      " statements . as with many microcap stocks , today ' s company has additiona |\n",
      "is report pertaining to investing , stocks , securities must be understood as \n",
      "ntative before deciding to trade in stocks featured within this report . none \n",
      "Displaying 2 of 2 matches:\n",
      "his email pertaining to investing , stocks , securities must be understood as \n",
      "ntative before deciding to trade in stocks featured within this email . none o\n",
      "Displaying 3 of 3 matches:\n",
      "might occur . as with many microcap stocks , today ' s company has additional \n",
      "is emai | pertaining to investing , stocks , securities must be understood as \n",
      "ntative before deciding to trade in stocks featured within this emai | . none \n",
      "Displaying 1 of 1 matches:\n",
      "in apple investments , inc profiled stocks . in order to be in full compliance\n",
      "Displaying 2 of 2 matches:\n",
      "his email pertaining to investing , stocks , securities must be understood as \n",
      "ntative before deciding to trade in stocks featured within this email . none o\n",
      "Displaying 1 of 1 matches:\n",
      " one trade monday ! go wysk . penny stocks are considered highiy specuiative a\n",
      "Displaying 1 of 1 matches:\n",
      " one trade monday ! go ndin . penny stocks are considered highly speculative a\n",
      "Displaying 2 of 2 matches:\n",
      " % on regular price we have massive stocks of drugs for same day dispatch fast\n",
      "e do have the lowest price and huge stocks ready for same - day dispatch . two\n",
      "Displaying 2 of 2 matches:\n",
      " % on regular price we have massive stocks of drugs for same day dispatch fast\n",
      "e do have the lowest price and huge stocks ready for same - day dispatch . two\n",
      "Displaying 1 of 1 matches:\n",
      "cautions that small and micro - cap stocks are high - risk investments and tha\n",
      "Displaying 4 of 4 matches:\n",
      "nt opportunity drummond , small cap stocks alert newsletter must read - alert \n",
      "his email pertaining to investing , stocks , securities must be understood as \n",
      "ntative before deciding to trade in stocks featured within this email . none o\n",
      " lose money from investing in penny stocks . - - - - - - - - - - - - - - - - -\n",
      "Displaying 6 of 6 matches:\n",
      " if you knew about these low priced stocks : otcbb : zapz : closed march 31 st\n",
      " following points : * many of these stocks are undiscovered and uncovered ! wh\n",
      " ! ! * * many of these undiscovered stocks are like coiled springs , wound tig\n",
      "might occur . as with many microcap stocks , today ' s company has additional \n",
      "his email pertaining to investing , stocks , securities must be understood as \n",
      "ntative before deciding to trade in stocks featured within this email . none o\n",
      "Displaying 2 of 2 matches:\n",
      " % on regular price we have massive stocks of drugs for same day dispatch fast\n",
      "e do have the lowest price and huge stocks ready for same - day dispatch . two\n",
      "Displaying 2 of 2 matches:\n",
      "ck monday some of these little voip stocks have been really moving lately . an\n",
      " one trade monday ! go ypil . penny stocks are considered highiy specuiative a\n",
      "Displaying 1 of 1 matches:\n",
      " receive first notice on run - away stocks traders ' monthly alert january pic\n",
      "Displaying 2 of 2 matches:\n",
      "his email pertaining to investing , stocks , securities must be understood as \n",
      "ntative before deciding to trade in stocks featured within this email . none o\n",
      "Displaying 3 of 3 matches:\n",
      "ancements but may be one of the few stocks left in this industry group that is\n",
      "his email pertaining to investing , stocks , securities must be understood as \n",
      "ntative before deciding to trade in stocks featured within this email . none o\n",
      "Displaying 2 of 2 matches:\n",
      "ck monday some of these little voip stocks have been realiy moving lately . an\n",
      " one trade monday ! go ypil . penny stocks are considered highiy specuiative a\n",
      "Displaying 1 of 1 matches:\n",
      " one trade monday ! go wysk . penny stocks are considered highiy specuiative a\n",
      "Displaying 1 of 1 matches:\n",
      "ne trade thursday ! go fcdh . penny stocks are considered highiy specuiative a\n",
      "Displaying 1 of 1 matches:\n",
      "s obtained . investing in micro cap stocks is extremely risky and , investors \n",
      "Displaying 1 of 1 matches:\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ penny - stocks are considered highly speculative a\n",
      "Displaying 1 of 1 matches:\n",
      "ecializing in undervalued small cap stocks for immediate breakout erhc and exx\n",
      "Displaying 2 of 2 matches:\n",
      "ims and do your own due diligence . stocks to play ( s 2 p ) profiles are not \n",
      "s obtained . investing in micro cap stocks is extremely risky and , investors \n",
      "Displaying 2 of 2 matches:\n",
      "is emai | pertaining to investing , stocks , securities must be understood as \n",
      "ntative before deciding to trade in stocks featured within this email . none o\n",
      "Displaying 3 of 3 matches:\n",
      " statements . as with many microcap stocks , todays company has additional ris\n",
      "blication pertaining to investing , stocks , securities must be understood as \n",
      "ntative before deciding to trade in stocks featured within this publication . \n",
      "Displaying 3 of 3 matches:\n",
      "his email pertaining to investing , stocks , securities must be understood as \n",
      "ntative before deciding to trade in stocks featured within this email . none o\n",
      " lose money from investing in penny stocks . if you wish to stop future mailin\n",
      "Displaying 3 of 3 matches:\n",
      "5 how many times have you seen good stocks but you couldn ' t get your hands o\n",
      "his email pertaining to investing , stocks , securities must be understood as \n",
      "ntative before deciding to trade in stocks featured within this email . none o\n",
      "Displaying 4 of 4 matches:\n",
      "n this stock . some of these smal | stocks are absoiuteiy fiying , as many of \n",
      " statements . as with many microcap stocks , todays company has additional ris\n",
      "biication pertaining to investing , stocks , securities must be understood as \n",
      "ntative before deciding to trade in stocks featured within this publication . \n",
      "Displaying 4 of 4 matches:\n",
      "his email pertaining to investing , stocks , securities must be understood as \n",
      "ntative before deciding to trade in stocks featured within this email . none o\n",
      "eep in mind that when trading small stocks like the company above there is a c\n",
      "t professional before investing any stocks or mutual funds .\n",
      "Displaying 1 of 1 matches:\n",
      " one trade monday ! go wysk . penny stocks are considered highiy speculative a\n",
      "Displaying 4 of 4 matches:\n",
      "hree days . play of the week tracks stocks on downward trends , foresees botto\n",
      "mark is our uncanny ability to spot stocks that have bottomed - out and antici\n",
      "ound and upward trend . most of the stocks we track rebound and peak within ju\n",
      "om third party . investing in penny stocks is high risk and you should seek pr\n",
      "Displaying 3 of 3 matches:\n",
      "orage inc . play of the week tracks stocks on downward trends , foresees botto\n",
      "his email pertaining to investing , stocks , securities must be understood as \n",
      "ntative before deciding to trade in stocks featured within this email . none o\n",
      "Displaying 1 of 1 matches:\n",
      "the | ast 12 months , many of these stocks made triple and even quadruple retu\n",
      "Displaying 4 of 4 matches:\n",
      "ck monday some of these little voip stocks have been rea | | y moving lately .\n",
      " statements . as with many microcap stocks , today ' s company has additiona |\n",
      "is report pertaining to investing , stocks , securities must be understood as \n",
      "ntative before deciding to trade in stocks featured within this report . none \n",
      "Displaying 3 of 3 matches:\n",
      "might occur . as with many microcap stocks , today ' s company has additiona |\n",
      "is emai | pertaining to investing , stocks , securities must be understood as \n",
      "ntative before deciding to trade in stocks featured within this email . none o\n",
      "Displaying 2 of 2 matches:\n",
      "subject : penny stocks are about timing nomad internationa\n",
      " one trade friday ! go ndin . penny stocks are considered highiy speculative a\n",
      "Displaying 4 of 4 matches:\n",
      "k tuesday some of these littie voip stocks have been reaily moving lateiy . an\n",
      " statements . as with many microcap stocks , today ' s company has additional \n",
      "is report pertaining to investing , stocks , securities must be understood as \n",
      "ntative before deciding to trade in stocks featured within this report . none \n",
      "Displaying 3 of 3 matches:\n",
      "might occur . as with many microcap stocks , today ' s company has additiona |\n",
      "his email pertaining to investing , stocks , securities must be understood as \n",
      "ntative before deciding to trade in stocks featured within this email . none o\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 1 of 1 matches:\n",
      "scovering value in natural resource stocks elgin resources ( elr - tsx ) extra\n",
      "Displaying 3 of 3 matches:\n",
      " plays . widespread gains in energy stocks are inflating the portfolios of agg\n",
      "st levels of the year , with energy stocks outperforming all other market sect\n",
      "utions that sma | | and micro - cap stocks are high - risk investments and tha\n",
      "Displaying 4 of 4 matches:\n",
      "watch this one trade . these little stocks can surprise in a big way sometimes\n",
      "might occur . as with many microcap stocks , today ' s company has additional \n",
      "his email pertaining to investing , stocks , securities must be understood as \n",
      "ntative before deciding to trade in stocks featured within this email . none o\n",
      "Displaying 1 of 1 matches:\n",
      "cautions that small and micro - cap stocks are high - risk investments and tha\n",
      "Displaying 5 of 5 matches:\n",
      "ck monday some of these littie voip stocks have been really moving lately . an\n",
      "t can happen with these sma | | cap stocks when they take off . and it happens\n",
      " statements . as with many microcap stocks , today ' s company has additiona |\n",
      "is report pertaining to investing , stocks , securities must be understood as \n",
      "ntative before deciding to trade in stocks featured within this report . none \n",
      "Displaying 3 of 3 matches:\n",
      "report reveals this smallcap rocket stocks newsletter first we would like to s\n",
      "his email pertaining to investing , stocks , securities must be understood as \n",
      "ntative before deciding to trade in stocks featured within this email . none o\n",
      "Displaying 4 of 4 matches:\n",
      " the last 12 months , many of these stocks made triple and even quadruple retu\n",
      " statements . as with many microcap stocks , today ' s company has additiona |\n",
      "is report pertaining to investing , stocks , securities must be understood as \n",
      "ntative before deciding to trade in stocks featured within this report . none \n",
      "Displaying 1 of 1 matches:\n",
      " the last 12 months , many of these stocks made tripie and even quadruple retu\n",
      "Displaying 3 of 3 matches:\n",
      "n how many times have you seen good stocks but you couldn ' t get your hands o\n",
      "his email pertaining to investing , stocks , securities must be understood as \n",
      "ntative before deciding to trade in stocks featured within this email . none o\n",
      "Displaying 1 of 1 matches:\n",
      "fessionally not multi - level - not stocks - not real estate no cost tele - se\n",
      "Displaying 3 of 3 matches:\n",
      "might occur . as with many microcap stocks , today ' s company has additiona |\n",
      "is emai | pertaining to investing , stocks , securities must be understood as \n",
      "ntative before deciding to trade in stocks featured within this email . none o\n",
      "Displaying 2 of 2 matches:\n",
      "his email pertaining to investing , stocks , securities must be understood as \n",
      "ntative before deciding to trade in stocks featured within this emai | . none \n",
      "Displaying 1 of 1 matches:\n",
      "or information puposes only . penny stocks are considered highly speculative a\n",
      "Displaying 2 of 2 matches:\n",
      "his email pertaining to investing , stocks , securities must be understood as \n",
      "ntative before deciding to trade in stocks featured within this email . none o\n",
      "Displaying 3 of 3 matches:\n",
      "might occur . as with many microcap stocks , today ' s company has additiona |\n",
      "is emai | pertaining to investing , stocks , securities must be understood as \n",
      "ntative before deciding to trade in stocks featured within this emai | . none \n",
      "Displaying 4 of 4 matches:\n",
      "tion is key to stock success rocket stocks newsletter u r g e n t i n v e s t \n",
      "ht occur . as with many micro - cap stocks , today ' s company has additional \n",
      "his email pertaining to investing , stocks , securities must be understood as \n",
      "ntative before deciding to trade in stocks featured within this email . none o\n",
      "Displaying 4 of 4 matches:\n",
      "his email pertaining to investing , stocks , securities must be understood as \n",
      "ntative before deciding to trade in stocks featured within this email . none o\n",
      "eep in mind that when trading small stocks like the company above there is a c\n",
      "t professional before investing any stocks or mutual funds .\n",
      "Displaying 4 of 4 matches:\n",
      "y agree , some , not all , of these stocks move in price because they are prom\n",
      "tands or that as with many microcap stocks , today ' s company has additional \n",
      "is report pertaining to investing , stocks , securities must be understood as \n",
      "ntative before deciding to trade in stocks featured within this report . none \n",
      "Displaying 2 of 2 matches:\n",
      "ng their gains . select gold mining stocks are the hot flyers of the otc . his\n",
      "is letter cautions that micro - cap stocks are high - risk investments and tha\n",
      "Displaying 3 of 3 matches:\n",
      " statements . as with many microcap stocks , today ' s company has additiona |\n",
      "is report pertaining to investing , stocks , securities must be understood as \n",
      "ntative before deciding to trade in stocks featured within this report . none \n",
      "Displaying 2 of 2 matches:\n",
      "rt identifying defense and security stocks ready to explode look at the moves \n",
      " actual exchanges where small - cap stocks are traded . silica stopband doorkn\n",
      "Displaying 1 of 1 matches:\n",
      "subject : fwd : screw doctors . stocks available . vlagr @ . x _ a _ nax .\n",
      "Displaying 5 of 5 matches:\n",
      "5 where were you when the following stocks exploded : scos : exploded from . 3\n",
      "d . 80 on friday . face it . little stocks can mean big gains for you . this r\n",
      "might occur . as with many microcap stocks , today ' s company has additional \n",
      "his email pertaining to investing , stocks , securities must be understood as \n",
      "ntative before deciding to trade in stocks featured within this report . none \n",
      "Displaying 6 of 6 matches:\n",
      "hem : ( big money was made in these stocks by savvy investors who timed them r\n",
      "g filthy , stinking ri ' ch in tiny stocks no one has ever heard of until now \n",
      "ynamic things . some of these small stocks have absolutely exploded in price r\n",
      "`` occur . as with many micro - cap stocks , today ' s company has additional \n",
      " ema - il pertaining to investing , stocks or securities must be understood as\n",
      "ntative before deciding to trade in stocks featured within this ema - il . non\n",
      "Displaying 2 of 2 matches:\n",
      " the last 12 months , many of these stocks made tripie and even quadruple retu\n",
      "one trade tuesday ! go mogi . penny stocks are considered highly speculative a\n",
      "Displaying 3 of 3 matches:\n",
      " statements . as with many microcap stocks , todays company has additional ris\n",
      "blication pertaining to investing , stocks , securities must be understood as \n",
      "ntative before deciding to trade in stocks featured within this publication . \n"
     ]
    }
   ],
   "source": [
    "\n",
    "from nltk.text import Text\n",
    "\n",
    "def concordance(data_list, search_word):\n",
    "    for email in data_list:\n",
    "        word_list = [word for word in word_tokenize(email.lower())]\n",
    "        text_list = Text(word_list)\n",
    "        if search_word in word_list:\n",
    "            # concordance checks for the occurrences of the specified word\n",
    "            # prints out word in context\n",
    "            # NLTK by default prints 36 characters either side of the search word\n",
    "            text_list.concordance(search_word)\n",
    "            \n",
    "print(\"STOCKS in HAM: \")\n",
    "concordance(ham_list, \"stocks\")\n",
    "print(\"\\n\\nSTOCKS in SPAM: \")\n",
    "concordance(spam_list, \"stocks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
