{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 2 of Getting Started with Natural Language Processing\n",
    "by Ekaterina Kochmar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing\n",
    "\n",
    "Looking at some methods to split textual data into meaningful words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using split to seperate whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Michael', 'Jordan', 'is', 'considered', 'by', 'many', 'to', 'be', 'the', 'greatest', 'basketball', 'player', 'of', 'all', 'time']\n"
     ]
    }
   ],
   "source": [
    "text = 'Michael Jordan is considered by many to be the greatest basketball player of all time'\n",
    "\n",
    "text_words = text.split(\" \")\n",
    "print(text_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Considering punctuation\n",
    "Punctuation will be considered as part of the word which changes the word representation as a feature as opposed to the *same* word that has no punctuation. This will cause issues for NLP and one which tokenization should handle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['How', 'should', 'we', 'regard', \"Jordan's\", '\"greatness\"', 'in', 'a', 'basketball', 'context?', 'Is', 'it', 'Greatness', 'at', 'some', 'or', 'all', 'facets', 'of', 'the', 'game?', 'Does', 'the', 'U.S.A', 'have', 'different', 'criteria', 'to', 'U.K?', 'First,', 'lets', 'consider', \"Jordan's\", '', 'greatness.', '', 'How', 'great', 'was', 'Jordan?', '', \"I'm\", 'going', 'to', 'say', 'he', 'was', 'great!', '', 'I', \"haven't\", 'met', 'anyone', 'who', 'disagrees.']\n"
     ]
    }
   ],
   "source": [
    "text = 'How should we regard Jordan\\'s \"greatness\" in a basketball context? Is it Greatness at \\\n",
    "some or all facets of the game? Does the U.S.A have different criteria to U.K? First, lets consider \\\n",
    "Jordan\\'s  greatness. '\n",
    "\n",
    "question = \"How great was Jordan? \"\n",
    "answer = \"I'm going to say he was great! \"\n",
    "answer2 = \"I haven't met anyone who disagrees.\"\n",
    "text = text +\" \"+ question +\" \"+ answer +\" \"+ answer2\n",
    "\n",
    "text_words = text.split(\" \")\n",
    "print(text_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In sentence above **\"greatness\"**, **Greatness** and **greatness** all have the same meaning but will be represented differenly as words. The capitalisation of words will be considered when we normalise textual data.\n",
    "\n",
    "Lets update the algorithm so that it considers puntuation when splitting the text into words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How should we regard Jordan\\'s \"greatness\" in a basketball context? Is it Greatness at some or all facets of the game? Does the U.S.A have different criteria to U.K? First, lets consider Jordan\\'s  greatness.  How great was Jordan?  I\\'m going to say he was great!  I haven\\'t met anyone who disagrees.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# List of words\n",
    "words = []\n",
    "\n",
    "# Track the current word\n",
    "current_word = \"\"\n",
    "\n",
    "# List of delimiters\n",
    "delimiters = ['\"', '.', \"?\", \"!\"]\n",
    "\n",
    "# iterate through each character in the text\n",
    "for c in text:\n",
    "    if len(current_word) > 0:\n",
    "        previous_char = current_word[-1]\n",
    "    \n",
    "    if c == \" \":\n",
    "        words.append(current_word)    # add the current_word to words list\n",
    "        current_word = \"\"             # initialise the current_word to empty string again\n",
    "    elif c in delimiters and previous_char != \" \":\n",
    "        words.append(current_word)\n",
    "        words.append(c)\n",
    "        current_word = \"\"\n",
    "    elif c in delimiters and previous_char == \" \":\n",
    "        words.append(c)\n",
    "        current_word = \"\"\n",
    "    else:\n",
    "        current_word += c \n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['How', 'should', 'we', 'regard', \"Jordan's\", '', '\"', 'greatness', '\"', '', 'in', 'a', 'basketball', 'context', '?', '', 'Is', 'it', 'Greatness', 'at', 'some', 'or', 'all', 'facets', 'of', 'the', 'game', '?', '', 'Does', 'the', 'U', '.', 'S', '.', 'A', 'have', 'different', 'criteria', 'to', 'U', '.', 'K', '?', '', 'First,', 'lets', 'consider', \"Jordan's\", '', 'greatness', '.', '', '', 'How', 'great', 'was', 'Jordan', '?', '', '', \"I'm\", 'going', 'to', 'say', 'he', 'was', 'great', '!', '', '', 'I', \"haven't\", 'met', 'anyone', 'who', 'disagrees', '.']\n"
     ]
    }
   ],
   "source": [
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From the book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['How', 'should', 'we', 'regard', \"Jordan's\", '\"', 'greatness', '\"', 'in', 'a', 'basketball', 'context', '?', 'Is', 'it', 'Greatness', 'at', 'some', 'or', 'all', 'facets', 'of', 'the', 'game', '?', 'Does', 'the', 'U', '.', 'S', '.', 'A', 'have', 'different', 'criteria', 'to', 'U', '.', 'K', '?', 'First,', 'lets', 'consider', \"Jordan's\", 'greatness', '.', 'How', 'great', 'was', 'Jordan', '?', \"I'm\", 'going', 'to', 'say', 'he', 'was', 'great', '!', 'I', \"haven't\", 'met', 'anyone', 'who', 'disagrees', '.']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "delimiters = ['\"', \".\", \"?\", \"!\"]\n",
    "words = []\n",
    "current_word = \"\"\n",
    " \n",
    "for char in text:\n",
    "    if char == \" \":\n",
    "        if not current_word == \"\":\n",
    "            words.append(current_word)\n",
    "            current_word = \"\"\n",
    "    elif char in delimiters:\n",
    "        if current_word == \"\":\n",
    "            words.append(char)\n",
    "        else:\n",
    "            words.append(current_word)\n",
    "            words.append(char)\n",
    "            current_word = \"\"\n",
    "    else:\n",
    "        current_word += char\n",
    "        \n",
    "    \n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are still some issues with the above code if they are to operate as tokenizers:\n",
    "* Can't handle abbreviations like \"U.K.\" or \"i.e.\"\n",
    "* Word concatenation like \"I'm\" will not be understood as \"I am\"\n",
    "    * Tokenizer would split the answer into [I, 'm, going, to , say, he ,was, great, !] and some rules would recognise 'm as short for \"am\" in this example\n",
    "    * Similarly \"haven't\" should be recognised as \"have not\". Tokenizer would split answer2 into [I have, n't, met, anyone, who, disagees, .]\n",
    "\n",
    "Good tokenizing packages ensure that text is properly split into words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enron case study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import codecs\n",
    "\n",
    "def read_in(folder):\n",
    "    files = os.listdir(folder)\n",
    "    a_list = []\n",
    "    for file in files:\n",
    "        if not file.startswith(\".\"):\n",
    "            f = codecs.open(os.path.join(folder,file), \"r\", \n",
    "                encoding=\"ISO-8859-1\", errors=\"ignore\")\n",
    "            a_list.append(f.read())\n",
    "            f.close()\n",
    "    return a_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500\n",
      "3672\n",
      "***SPAM***\n",
      "Subject: office xp - $60 convenes permian\r\n",
      "Even is story. Live together store went where grass. Pull, your,\r\n",
      "Large. Yes then north, stick said. Number produce thing, hair\r\n",
      "Corner, play, basic. Air may figure. View house then which,\r\n",
      "Ready. Collect clear with. Wide arm what, industry, sound, clear\r\n",
      "Talk. Job distant, keep fit. Sense quick would seem, take. I\r\n",
      "Row, and. Sound, decimal color work coast friend. Ride east sand\r\n",
      "Took. Believe him then, modern, old catch like.\r\n",
      "\n",
      "***HAM***\n",
      "Subject: on - call notes\r\n",
      "Please see the attached file for on - call notes.\r\n",
      "Bob\n"
     ]
    }
   ],
   "source": [
    "data_path = 'data/enron1/'\n",
    "\n",
    "# each element in the list contains contents of one email\n",
    "ham_list = read_in(os.path.join(data_path, 'ham'))\n",
    "spam_list = read_in(os.path.join(data_path, 'spam'))\n",
    "\n",
    "print(len(spam_list))\n",
    "print(len(ham_list))\n",
    "print(\"***SPAM***\")\n",
    "print(spam_list[2])\n",
    "print(\"***HAM***\")\n",
    "print(ham_list[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing the data\n",
    "Combine the ham/spam datasets, include the label then shuffle the final dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size = 5172 emails\n"
     ]
    }
   ],
   "source": [
    "import random \n",
    "\n",
    "all_emails = [(email_content, \"spam\") for email_content in spam_list]\n",
    "all_emails += [(email_content, \"ham\") for email_content in ham_list]\n",
    "random.seed(42)\n",
    "random.shuffle(all_emails)\n",
    "print(f\"Dataset size = {str(len(all_emails))} emails\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizing\n",
    "Emails are single string of symbols so need to split the text into words. Use NLTK's tokenizer for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/js/repos/hosting-ml-as-\n",
      "[nltk_data]     microservice/venv/share/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk \n",
    "\n",
    "nltk.download('punkt', download_dir='/home/js/repos/hosting-ml-as-microservice/venv/share/nltk_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['What', \"'s\", 'the', 'best', 'way', 'to', 'split', 'a', 'sentence', 'into', 'words', '?']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from nltk import word_tokenize\n",
    "\n",
    "def tokenize(input):\n",
    "    word_list = []\n",
    "    for word in word_tokenize(input):\n",
    "        word_list.append(word)\n",
    "    return word_list\n",
    "\n",
    "input = \"What's the best way to split a sentence into words?\"\n",
    "print(tokenize(input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract and normalise the features\n",
    "* Iterate over emails and tokenize the text that has been tranformed to be lower case so that it is normalised. \n",
    "* Each email and label are paired together in tuples.\n",
    "* Tokenized list of words are converted to dictionary with words as keys and set to value of True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(text):\n",
    "    features = {}\n",
    "    word_list = [word for word in word_tokenize(text.lower())]\n",
    "    for word in word_list:\n",
    "        features[word] = True\n",
    "    return features\n",
    "\n",
    "all_features = [(get_features(email), label) for (email, label) in all_emails]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Subject: how to confidently attract, meet and seduce more hot women\\r\\nFact: you can go out tonight and approach\\r\\nAny beautiful woman confidently and without fear. You can know\\r\\nExactly what to say to break the ice...... And exactly\\r\\nWhat to do to get her into bed!\\r\\nRmove\\r\\n',\n",
       " 'spam')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_emails[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'subject': True, ':': True, 'how': True, 'to': True, 'confidently': True, 'attract': True, ',': True, 'meet': True, 'and': True, 'seduce': True, 'more': True, 'hot': True, 'women': True, 'fact': True, 'you': True, 'can': True, 'go': True, 'out': True, 'tonight': True, 'approach': True, 'any': True, 'beautiful': True, 'woman': True, 'without': True, 'fear': True, '.': True, 'know': True, 'exactly': True, 'what': True, 'say': True, 'break': True, 'the': True, 'ice': True, '......': True, 'do': True, 'get': True, 'her': True, 'into': True, 'bed': True, '!': True, 'rmove': True}, 'spam')\n"
     ]
    }
   ],
   "source": [
    "print(all_features[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'subject': True, ':': True, 'how': True, 'to': True, 'confidently': True, 'attract': True, ',': True, 'meet': True, 'and': True, 'seduce': True, 'more': True, 'hot': True, 'women': True, 'fact': True, 'you': True, 'can': True, 'go': True, 'out': True, 'tonight': True, 'approach': True, 'any': True, 'beautiful': True, 'woman': True, 'without': True, 'fear': True, '.': True, 'know': True, 'exactly': True, 'what': True, 'say': True, 'break': True, 'the': True, 'ice': True, '......': True, 'do': True, 'get': True, 'her': True, 'into': True, 'bed': True, '!': True, 'rmove': True}\n",
      "----------\n",
      "label: spam\n"
     ]
    }
   ],
   "source": [
    "print(all_features[0][0])\n",
    "print(\"----------\")\n",
    "print(f\"label: {all_features[0][1]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
