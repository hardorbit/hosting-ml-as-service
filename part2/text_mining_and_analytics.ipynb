{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Mining and Analytics\n",
    "\n",
    "Text mining is used for analyzing texts, turns them into a more structured form and then deriving insights from it \n",
    "\n",
    "#### Example technique\n",
    "\n",
    "**named entity recognition:** Subtask of information extraction that locates and calssifies *named entities*, mentioned in *unstructured text*, into predefined categories like:\n",
    "* names of people \n",
    "* locations\n",
    "* dates\n",
    "* ID codes\n",
    "\n",
    "e.g. Unstrucutred text --> named entity recognition --> annotated text\n",
    "\n",
    ">Jim bought 300 shares of Acme Corp. in 2006.\n",
    "\n",
    "And producing an annotated block of text that highlights the names of entities:\n",
    "\n",
    ">**Jim** bought 300 shares of **Acme Corp.** in **2006**\n",
    "\n",
    "Jim: Person  \n",
    "Acme Corp: Organization  \n",
    "2006: Time  \n",
    "\n",
    "#### Applications\n",
    "* Entity identification\n",
    "* Plagiarism detection\n",
    "* Topic identification\n",
    "* Text clustering\n",
    "* Translation\n",
    "* Auto-text summarisation\n",
    "* Fraud detection\n",
    "* Spam filtering\n",
    "* Sentiment analysis\n",
    "\n",
    "#### Difficulties\n",
    "* Text ambiguity\n",
    "* Spelling mistakes \n",
    "* Acronyms\n",
    "* Homonyms: bat (animal) <--> bat (baseball)\n",
    "* Language context: \n",
    "    * Engligh models of langage won't work well for Arabic and vice versa\n",
    "    * Algorithm trained on Twitter data won't work well if applied to legal texts\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words\n",
    "\n",
    "Simplest way to structure textual data --> evey document turned into a word vector. If word is a boolean defined by condition that word is contained in document.\n",
    "    \n",
    "Consider two documents:\n",
    "\n",
    "> 1. \"Game of thrones is a great television series but the books are better.\"  \n",
    " \n",
    "> 2. \"Doing data science is more fun that watching television\"\n",
    "\n",
    "Can combine the documents into a structured format called the *document-term matrix*:\n",
    "* Rows --> documents\n",
    "* Columns --> words\n",
    "\n",
    "Binary coded bag of words is one to structure the data, others exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  [\n",
      "    {\n",
      "      \"game\": true,\n",
      "      \"of\": true,\n",
      "      \"thrones\": true,\n",
      "      \"is\": true,\n",
      "      \"a\": true,\n",
      "      \"great\": true,\n",
      "      \"television\": true,\n",
      "      \"series\": true,\n",
      "      \"but\": true,\n",
      "      \"the\": true,\n",
      "      \"books\": true,\n",
      "      \"are\": true,\n",
      "      \"better\": true,\n",
      "      \"doing\": false,\n",
      "      \"data\": false,\n",
      "      \"science\": false,\n",
      "      \"more\": false,\n",
      "      \"fun\": false,\n",
      "      \"than\": false,\n",
      "      \"watching\": false\n",
      "    },\n",
      "    \"gameofthrones\"\n",
      "  ],\n",
      "  [\n",
      "    {\n",
      "      \"doing\": true,\n",
      "      \"data\": true,\n",
      "      \"science\": true,\n",
      "      \"is\": true,\n",
      "      \"more\": true,\n",
      "      \"fun\": true,\n",
      "      \"than\": true,\n",
      "      \"watching\": true,\n",
      "      \"television\": true,\n",
      "      \"game\": false,\n",
      "      \"of\": false,\n",
      "      \"thrones\": false,\n",
      "      \"a\": false,\n",
      "      \"great\": false,\n",
      "      \"series\": false,\n",
      "      \"but\": false,\n",
      "      \"the\": false,\n",
      "      \"books\": false,\n",
      "      \"are\": false,\n",
      "      \"better\": false\n",
      "    },\n",
      "    \"datascience\"\n",
      "  ]\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import json \n",
    "\n",
    "a = [({'game':True,'of':True,'thrones':True,'is':True,'a':True,\n",
    "'great':True,'television':True,'series':True,'but':True,\n",
    "'the':True,'books':True,'are':True,'better':True,'doing':\n",
    "False, 'data':False,'science':False,'more':False,'fun':False,\n",
    "'than':False,'watching':False},\n",
    "'gameofthrones'),\n",
    "({'doing':True,'data':True,'science':True,'is':True,'more':\n",
    "True,'fun':True,'than':True,'watching':True,'television':True,\n",
    "'game':False,'of':False,'thrones':False,'a':False,'great':\n",
    "False,'series':False,'but':False,'the':False,'books':False,\n",
    "'are':False,'better':False},\n",
    "'datascience')]\n",
    "\n",
    "print(json.dumps(a, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleansing Text Data\n",
    "\n",
    "Bunch of standard cleansing tasks to do before building document-term matrix:\n",
    "* normalising --> lowercase all text\n",
    "* tokenization --> cut text into pieces called tokens or terms\n",
    "    * lots of mechanisms for this\n",
    "    * consider token as a unit of information\n",
    "        * unigrams: single word tokens\n",
    "        * bigrams: two words per token\n",
    "        * n-grams n words per token\n",
    "* stop word filtering\n",
    "    * NLTK comes with list of English stop words \n",
    "    \n",
    "#### Stemming and lemmatisation\n",
    "\n",
    "*Stemming* is the process of bringing words back to their root form which can be things like removing suffixes. Some examples\n",
    "* [courses, course] --> course\n",
    "* [announce, announces, announcing] --> announce  \n",
    "    \n",
    "*Lemmatisation* definition from Wiki:\n",
    "\n",
    "\n",
    ">In linguistics, lemmatisation is the process of grouping together the inflected forms of a word so they can be analysed as a single item, identified by the word's lemma, or dictionary form \n",
    ">\n",
    "> 1. The word \"better\" has \"good\" as its lemma. This link is missed by stemming, as it requires a dictionary look-up.\n",
    ">\n",
    "> \n",
    "> 2. The word \"walk\" is the base form for the word \"walking\", and hence this is matched in both stemming and lemmatisation.\n",
    ">\n",
    ">\n",
    "> 3. The word \"meeting\" can be either the base form of a noun or a form of a verb (\"to meet\") depending on the context; e.g., \"in our last meeting\" or \"We are meeting again tomorrow\". Unlike stemming, lemmatisation attempts to select the correct lemma depending on the context.\n",
    "\n",
    "Transforming words to their lemma form benefits from Part of Speech Tagging aka POS Tagging. This process attributes a grammatical label to every part of a sentence. For example\n",
    "\n",
    ">Game of thrones is a television series\n",
    "\n",
    "is tagged as \n",
    "\n",
    "```\n",
    "{\n",
    "    'game':        'NN',   # Noun, singular or mass\n",
    "    'of':          'IN',   # Preposition or subordinating conjunction\n",
    "    'thrones':     'NNS',  # Noun, plural\n",
    "    'is':          'VBZ',  # Verb, 3rd person singular present\n",
    "    'a':           'DT',   # Determiner\n",
    "    'television':  'NN',   # Noun, singular or mass\n",
    "    'series':      'NN'    # Noun, singular or mass\n",
    "}\n",
    "```\n",
    "\n",
    "POS Tagging is a use case of sentence tokenisation rather than word-tokenisation.\n",
    "\n",
    "Lets consider how to build the document-term matrix\n",
    "\n",
    "### TF-IDF\n",
    "\n",
    "#### Term frequency\n",
    "\n",
    "Instead of binary encoding each word in a document we can count the number of times each word occurs in the document  \n",
    "\n",
    "> $TF(t,d) = f_{t,d}$ \n",
    "\n",
    "where $f_{t,d}$ is the frequency of each term in the document. \n",
    "\n",
    "If corpus contains lots of documents of different length then normalise the raw counts by document length. Example the word \"stock\" could occur in both a 200 word email and 20 word email c.f. Enron emails case study. Count could be 20 in 200 word email and 2 in 20 word email, normalised they're the same proportion of document (10%). Count can also be scaled logarithmically. \n",
    "\n",
    "#### Inverse Document Frequency\n",
    "\n",
    "Indicates how common the word is in the entire corpus (collection of documents).\n",
    "* Terms with **higher document frequency** $\\implies$ won't be good for discriminating when clustering documents\n",
    "* Terms with **low document frequency** $\\implies$ won't be basis for meaningful cluster\n",
    "\n",
    "Has a functional form that captures the distribution of the term over the entire corpus  \n",
    "\n",
    "> $IDF(t,D) = 1 + log_{2}\\left(\\frac{N}{|d \\in D:t \\in d|}\\right)$\n",
    "\n",
    "where N is number of documents in corpus and $N_{t}$ is the number of documents containing term *t*. Note a couple of things:\n",
    "* If term is present in every doc then IDF is 1\n",
    "* $N / N_{t}$ is the inverse document frequency\n",
    "* Alternative way to express number of documents containing term t, is $|d \\in D:t \\in d|$\n",
    "* There are alternative weighting schemes available\n",
    "\n",
    "#### TF-IDF\n",
    "\n",
    "This is calculated as \n",
    "\n",
    "> $TFIDF(t,d,D) = TF(t,d) \\cdot IDF(t,D)$\n",
    "\n",
    "A high weight in TF-IDF is reached by a high term frequency (in the given document) and a low document frequency of the term in the whole collection of documents; the weights hence tend to filter out common terms.\n",
    "\n",
    "#### Note on Naive part of Naive Bayes classifier\n",
    "\n",
    "In NB classifier each element in the evidence vector is considered indendentent which decomposes the vector when conditioned on $C = c_{1}$ into \n",
    "\n",
    ">$P(E|C=c_{1}$ = $P(e_{1}|c_{1})P(e_{2}|c_{1})...P(e_{k}|c_{1})$\n",
    "\n",
    "This is Naive because words like ```Data Sceince```, ```data analysis``` and ```Game of Thrones``` will not be linked if data is prepared as unigrams. Would need to consider bigrams, trigrams etc to consider the  word *interactions*. \n",
    "> $\\therefore$ Good to compare the NB against the decision tree classifier as it *does* consider variable interactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying reddit posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
